{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "class Shakespeare:\n",
    "    def __init__(self):\n",
    "        path_to_shakespeare = \"shakespeare_input.txt\"\n",
    "        with open(path_to_shakespeare, \"r\") as f:\n",
    "            self.shakespeare_text = f.read()\n",
    "\n",
    "    @staticmethod\n",
    "    def unzip(pairs):\n",
    "        \"\"\"\n",
    "        Splits list of pairs (tuples) into separate lists.\n",
    "\n",
    "        Example: pairs = [(\"a\", 1), (\"b\", 2)] --> [\"a\", \"b\"] and [1, 2]\n",
    "        \"\"\"\n",
    "        return tuple(zip(*pairs))\n",
    "    \n",
    "\n",
    "    def normalize(self, counter):\n",
    "        \"\"\" \n",
    "        Convert counter to a list of (letter, frequency) pairs, sorted in descending order of frequency.\n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        counter: A Counter-instance\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        A list of tuples - (letter, frequency) pairs. \n",
    "\n",
    "        \"\"\"\n",
    "        total = sum(counter.values())\n",
    "        return [(char, cnt/total) for char, cnt in counter.most_common()]\n",
    "    \n",
    "    \n",
    "    def train_lm(self, n):\n",
    "        \"\"\" \n",
    "        Train character-based n-gram language model.\n",
    "        \n",
    "        Given a sequence of n-1 characters, model will learn what the probability\n",
    "        distribution is for the n-th character in the sequence.\n",
    "\n",
    "        Tildas (\"~\") are used for padding the history when necessary, so that it's \n",
    "        possible to estimate the probability of a seeing a character when there \n",
    "        aren't (n - 1) previous characters of history available.\n",
    "           \n",
    "        Parameters\n",
    "        -----------\n",
    "        text: str \n",
    "            A string (doesn't need to be lowercased).\n",
    "        n: int\n",
    "            The length of n-gram to analyze.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        A dict that maps histories (strings of length (n-1)) to lists of (char, prob) \n",
    "        pairs, where prob is the probability of char appearing after \n",
    "        that specific history. \n",
    "\n",
    "        \"\"\"\n",
    "        raw_lm = defaultdict(Counter)\n",
    "        history = \"~\" * (n - 1)\n",
    "\n",
    "        # count number of times characters appear following different histories\n",
    "        for x in self.shakespeare_text:\n",
    "            raw_lm[history][x] += 1\n",
    "            history = history[1:] + x\n",
    "\n",
    "        # create final dictionary by normalizing\n",
    "        lm = { history : self.normalize(counter) for history, counter in raw_lm.items() }\n",
    "\n",
    "        return lm\n",
    "\n",
    "    def generate_letter(self, lm, history):\n",
    "        \"\"\" \n",
    "        Randomly picks letter according to probability distribution associated with \n",
    "        the specified history.\n",
    "\n",
    "        Note: returns dummy character \"~\" if history not found in model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        lm: Dict[str, Tuple[str, float]] \n",
    "            The n-gram language model. I.e. the dictionary: history -> (char, freq)\n",
    "\n",
    "        history: str\n",
    "            A string of length (n-1) to use as context/history for generating \n",
    "            the next character.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            The predicted character. '~' if history is not in language model.\n",
    "        \"\"\"\n",
    "        if not history in lm:\n",
    "            return \"~\"\n",
    "        letters, probs = Shakespeare.unzip(lm[history])\n",
    "        \n",
    "        i = np.random.choice(letters, p=probs)\n",
    "        return i\n",
    "\n",
    "    def generate_text(self, lm, n, nletters=100):      \n",
    "        \"\"\" \n",
    "        Randomly generates nletters of text with n-gram language model lm.\n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        lm: Dict[str, Tuple[str, float]] \n",
    "            The n-gram language model. I.e. the dictionary: history -> (char, freq)\n",
    "        n: int\n",
    "            Order of n-gram model.\n",
    "        nletters: int\n",
    "            Number of letters to randomly generate.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            Model-generated text.\n",
    "        \"\"\"\n",
    "        history = \"~\" * (n - 1)\n",
    "        text = []\n",
    "        for i in range(nletters):\n",
    "            c = self.generate_letter(lm, history)\n",
    "            text.append(c)\n",
    "            history = history[1:] + c\n",
    "        return \"\".join(text)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fireast se, weak you fall mand ow hiscell sir son seavere amen th me le\n",
      "Ay, lings cromford's manglame, Tyrace a cons\n",
      "he faid le belf modees his and to a prign yied, pas\n",
      "At men isest in'd\n",
      "And a the werevell.\n",
      "\n",
      "MET:\n",
      "Ay, I ando onan: to he thave his Slad, say fory.\n",
      "Dow, see, lost pard,\n",
      "My thaturdeepear sold.\n",
      "\n",
      "And cous: bray:\n",
      "So theacour rome ell dery\n",
      "Thaterseen was wilf'd sed my did offorou wither came earminese noth any.\n",
      "\n",
      "Laesight whe and me youre sne.\n",
      "No!\n",
      "Art's can's i' to hatones,\n",
      "The nown of a s\n"
     ]
    }
   ],
   "source": [
    "model = Shakespeare()\n",
    "lm3 = model.train_lm(3)\n",
    "print(model.generate_text(lm3, 3, 500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
